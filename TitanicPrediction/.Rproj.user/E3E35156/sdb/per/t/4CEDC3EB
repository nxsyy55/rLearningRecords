{
    "contents" : "---\ntitle: \"ReadMe\"\nauthor: \"BlackPluto\"\ndate: \"August 2, 2015\"\noutput: html_document\n---\n## The pratice on Kaggle\nRead and prepare the data first.\n```{r}\nsuppressMessages(library(ggplot2))\nsuppressMessages(library(dplyr))\ntrain <- read.csv('train.csv')\n# check\nsummary(train)\nhist(train$Age)\nshapiro.test(train$Age)\nqqnorm(train$Age)\nmedian(train$Age, na.rm = T)\ntrain_shaped <- train %>% select(-PassengerId, -Name, -Ticket, -Cabin) %>% \n                mutate(age = ifelse(is.na(Age), 28, Age), \n                       survive = factor(ifelse(Survived == 1, 'Yes', 'No' )), \n                                        class = factor(ifelse(Pclass == 1, 'U',\n                                                       ifelse(Pclass == 2, 'M', 'L')))) %>% select(-Survived, -Pclass, -Age)\n```\n## the base model, the logistic regression\nUse all the variables except the name and cabin. And __here only the complete cases are used.__ And I got about 80% accuracy.\n\n```{r}\nstr(train_shaped)\nbase_mod <- glm(data = train_shaped, survive ~ ., family = binomial)\nsummary(base_mod)\nbase_prediction <- ifelse(predict(base_mod, type = 'response') > 0.5, 'Yes', 'No')\ntable(train_shaped$survive, base_prediction)\n(base_accuracy <- (478 + 240) / 891) \n# decrease the variables\nbase_mod_min <- glm(data = train_shaped, survive ~ Sex + class + age + SibSp, family = binomial)\nbase_min_prediction <- ifelse(predict(base_mod_min, type = 'response') > 0.5, 'Yes', 'No')\ntable(train_shaped$survive, base_min_prediction)\n(base_accuracy <- (459 + 243) / 891) #78%\n# test with the test data set\ntest <- read.csv('test.csv')\ntest_shaped <- test %>% select(-PassengerId, -Name, -Ticket, -Cabin) %>% \n                mutate(age = ifelse(is.na(Age), 28, Age), \n                                    class = factor(ifelse(Pclass == 1, 'U',\n                                                   ifelse(Pclass == 2, 'M', 'L')))) %>% select(-Pclass, -Age)\nbase_test_result <- ifelse(predict(base_mod_min, newdata = test_shaped) >= 0.5, 1, 0)\nwrite.csv('base_mod_result.csv', x = data.frame(PassengerId = test$PassengerId, Survived = base_test_result), row.names = F)\n```\n### The base model result\n__Survive ~ Sex + class + age + SibSp__ Got 77.033% right on the test set. Now try the randomforest.\n```{r}\nlibrary(randomForest)\nrf_mod <- randomForest(data = train_shaped, survive ~ .)\n#predict(rf_mod, test_shaped)\n```\n",
    "created" : 1438522774503.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1107217426",
    "id" : "4CEDC3EB",
    "lastKnownWriteTime" : 1438609760,
    "path" : "/Volumes/MAC/Documents/R/TitanicPrediction/ReadMe.Rmd",
    "project_path" : "ReadMe.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_markdown"
}